## [PH4.1-11] Metrics par endpoint (Prom) + traces corrÃ©lÃ©es

Issue: PH4.1-11
Branch: feat/PH4.1-11-metrics-par-endpoint-prom-traces-corr-l-es

## Contexte
Exposer `http_server_requests_seconds_bucket` et `http_server_requests_total` par route/method/status et propager `trace_id` via `X-Trace-ID` et logs structurÃ©s.

## Scope (STRICT)
- ImplÃ©mentation observability au gateway (mÃ©triques HTTP par endpoint, traces corrÃ©lÃ©es).
- Hors scope: refactors transverses, timeouts/backoff, quotas.

## ImplÃ©mentation
- Middlewares dans `backend/app/main.py`:
  - `TraceIdMiddleware`, `RequestLoggingMiddleware` (avant protections)
  - `HTTPServerMetricsMiddleware` (labels: route/method/status)
  - `PrometheusMiddleware` et route `/metrics` (dÃ©jÃ  existants)
- Normalisation de route via `backend.app.metrics.normalize_route`.
- Aucun changement de contrat API; ajout de lâ€™en-tÃªte `X-Trace-ID` en rÃ©ponse.

## Tests
- `tests/test_metrics.py`: expose `/metrics` et prÃ©sence `http_server_requests_total`.
- `tests/apigw/test_http_metrics.py`: succÃ¨s/500, mÃ©thodes HTTP, normalisation, params ignorÃ©s, durÃ©e, trace ID, concurrence, cohÃ©rence des labels.

## CI Gates
- Ruff strict âœ…, mypy strict âœ…, pytest âœ…
- Ban typing aliases âœ… (aucun `typing.List/Dict/Optional/Str`)

## Risques & Rollback
1) SymptÃ´mes: cardinalitÃ© labels, surcoÃ»t latence middleware
2) Action: revert PR
3) VÃ©rification: `/metrics` normal, logs OK

## Checklist
- [x] Scope respectÃ©
- [x] Ruff + mypy OK
- [x] Aucun typing alias
- [x] Tests verts
- [x] PR ne rÃ©fÃ©rence quâ€™une seule issue

# PH4.1-10 â€” Suivi ops : buckets SLO, retries (mÃ©triques & rÃ¨gles), 410 headers, labels unifiÃ©s

## ğŸ¯ RÃ©sumÃ©
- **Buckets SLO-like** : http_server_requests_seconds_bucket = [0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0, 2.0].
- **MÃ©triques retry** :
  - pigw_retry_attempts_total{route,result="allowed|blocked"}
  - pigw_retry_budget_exhausted_total{route}
- **Classification retry durcie** :
  - **Retry autorisÃ©s** : timeouts + 5xx transitoires (502/503), **bornÃ©s** par le budget.
  - **Pas de retry** : 4xx (400/401/403/409) et **429**.
- **410 Sunset** : Cache-Control: max-age=60, must-revalidate (aucun Retry-After).
  - **Option CDN** : Surrogate-Control: max-age=60 **uniquement** si APIGW_EDGE_SURROGATE=1|true|yes|on.
- **Labels de route unifiÃ©s** : usage central de 
ormalize_route (metrics HTTP, rate-limit, auth, store).

## âœ… Tests ajoutÃ©s
- **4xx/429** : zÃ©ro retry (aucun incrÃ©ment â€¦{result="allowed"}).
- **502/503** : retry **jusquâ€™au budget** + Ã©mission de â€¦_budget_exhausted_total et â€¦{result="blocked"} Ã  lâ€™Ã©puisement.
- **500** : garde **single-increment** (pas de double comptage http_server_requests_total).
- **CDN gating** : Surrogate-Control prÃ©sent **uniquement** si env activÃ©.

## ğŸ§ª QualitÃ©
- Ruff/mypy strict âœ… â€” **types natifs uniquement** (aucun 	yping.List/Dict/Optional/Str).
- Couverture **apigw â‰¥ 90%** âœ….
- Buckets alignÃ©s SLO, **aucun risque de cardinalitÃ©**.

## ğŸš€ Rollout
- **Canary 10%** avec SLO Gate, surveillance 30â€“60 min :
  - histogram_quantile(0.95, sum(rate(http_server_requests_seconds_bucket[5m])) by (le,route))
  - sum(rate(apigw_retry_attempts_total{result="allowed"}[5m])) by (route)
  - sum(rate(apigw_retry_budget_exhausted_total[15m])) by (route)
- **Promotion 100%** si RAS.

## ğŸ“‹ Checklist merge/rollout
- [ ] Lints/types/tests **verts** ; **coverage apigw â‰¥ 90%**.
- [ ] **SLO Gate** OK.
- [ ] Dashboards Ã  jour : panels **Retry Allowed**, **Retry Budget**, **HTTP P95/P99**, **ratio 5xx**.
- [ ] Canary 10% : alerte si â€¦_budget_exhausted_total soutenu ou P95 en hausse ; promotion si stable.

## ğŸ”œ Follow-ups (petites PR dÃ©diÃ©es)
- **E2E idempotent POST + retry** : vÃ©rifier retour **mÃªme status+body** via cache idempotent.
- **Property-based â€œcross-stack labelsâ€** : un **seul** label 
oute pour /v1/chat/123?x=1&y=2 (metrics / RL / versioning).
- **Docs** docs/api/timeouts_retries.md : tableau par endpoint (timeouts, erreurs retryables, budget/jitter) + note CDN.
