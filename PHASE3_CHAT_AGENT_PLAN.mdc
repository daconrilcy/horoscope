
# Title
Phase 3 — Chat Conseiller (Retrieval & LLM) — Implementation Plan for VS Code GPT Agent

# Summary
Implement the Phase 3 features described in `backend_v2.1_review_final.md`:
- Retrieval Engine (semantic search + indexing)
- Chat Orchestration with LLM (advice generation)
- Async infrastructure for heavy jobs (Celery/Redis)
- Monitoring upgrades (Prometheus + traces)
- Tests (deterministic) and DX guardrails
- Keep all imports as `from backend...`

# Preconditions
- Existing backend layout: `backend/{api,app,core,domain,infra,middlewares,scripts,tests}`.
- Phase 2 merged (Auth/JWT, Entitlements, Redis repos, PDF, Prometheus).
- Docker Compose with `redis` service.
- You may edit `requirements.txt`, `.env.example`, Docker files.

# Milestones
1) Dependencies & settings
2) Embeddings + Vector Store abstraction
3) Retrieval engine (index + query)
4) Chat orchestration (LLM client + prompt)
5) API routes (chat) + entitlement gating
6) Async infra (Celery worker) + tasks
7) Monitoring (metrics + traces)
8) Ingestion script
9) Tests (unit + e2e with fakes)
10) Docker & env wiring
11) Acceptance checklist

# 1) Dependencies & Settings

## Edit: requirements.txt (append if missing)
```
openai>=1.43.0
sentence-transformers>=3.0.1
faiss-cpu>=1.8.0
celery>=5.4.0
tenacity>=8.5.0
opentelemetry-sdk>=1.26.0
opentelemetry-instrumentation-fastapi>=0.47b0
opentelemetry-exporter-otlp>=1.26.0
```
> Option: If using Elasticsearch instead of FAISS, add `elasticsearch>=8.13.0` and skip FAISS.

## Edit: backend/core/settings.py (append fields)
```python
class Settings(BaseSettings):
    ...
    OPENAI_API_KEY: str | None = None
    EMBEDDINGS_PROVIDER: str = "openai"  # "openai" | "local"
    EMBEDDINGS_MODEL: str = "text-embedding-3-small"  # if openai
    LOCAL_EMBEDDINGS_MODEL: str = "all-MiniLM-L6-v2"  # if local
    VECTOR_BACKEND: str = "faiss"  # "faiss" | "elasticsearch"
    OTLP_ENDPOINT: str | None = None  # e.g. http://otel-collector:4317
    CELERY_BROKER_URL: str = "redis://redis:6379/0"
    CELERY_RESULT_BACKEND: str = "redis://redis:6379/1"
```

## Edit: .env.example
```
OPENAI_API_KEY=
EMBEDDINGS_PROVIDER=openai
EMBEDDINGS_MODEL=text-embedding-3-small
LOCAL_EMBEDDINGS_MODEL=all-MiniLM-L6-v2
VECTOR_BACKEND=faiss
OTLP_ENDPOINT=
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/1
```

# 2) Embeddings + Vector Store abstraction

## New: backend/domain/retrieval_types.py
```python
from pydantic import BaseModel
from typing import List, Dict, Any

class Document(BaseModel):
    id: str
    text: str
    meta: Dict[str, Any] = {}

class Query(BaseModel):
    text: str
    k: int = 5

class ScoredDocument(BaseModel):
    doc: Document
    score: float
```

## New: backend/infra/embeddings/base.py
```python
from abc import ABC, abstractmethod
from typing import List

class Embeddings(ABC):
    @abstractmethod
    def embed(self, texts: List[str]) -> List[List[float]]: ...
```

## New: backend/infra/embeddings/openai_embedder.py
```python
from typing import List
from backend.core.container import container
from backend.infra.embeddings.base import Embeddings
from openai import OpenAI

class OpenAIEmbedder(Embeddings):
    def __init__(self):
        self.client = OpenAI(api_key=container.settings.OPENAI_API_KEY)
        self.model = container.settings.EMBEDDINGS_MODEL

    def embed(self, texts: List[str]) -> List[List[float]]:
        resp = self.client.embeddings.create(model=self.model, input=texts)
        return [d.embedding for d in resp.data]
```

## New: backend/infra/embeddings/local_embedder.py
```python
from typing import List
from backend.infra.embeddings.base import Embeddings
from sentence_transformers import SentenceTransformer

class LocalEmbedder(Embeddings):
    _model = None

    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        if LocalEmbedder._model is None:
            LocalEmbedder._model = SentenceTransformer(model_name)
        self.model = LocalEmbedder._model

    def embed(self, texts: List[str]) -> List[List[float]]:
        return self.model.encode(texts, convert_to_numpy=True).tolist()
```

## New: backend/infra/vecstores/base.py
```python
from abc import ABC, abstractmethod
from typing import List, Tuple
from backend.domain.retrieval_types import Document, ScoredDocument, Query

class VectorStore(ABC):
    @abstractmethod
    def index(self, docs: List[Document]) -> int: ...
    @abstractmethod
    def search(self, q: Query) -> List[ScoredDocument]: ...
```

## New: backend/infra/vecstores/faiss_store.py
```python
import faiss, numpy as np
from typing import List, Dict
from backend.infra.vecstores.base import VectorStore
from backend.domain.retrieval_types import Document, ScoredDocument, Query
from backend.core.container import container
from backend.infra.embeddings.openai_embedder import OpenAIEmbedder
from backend.infra.embeddings.local_embedder import LocalEmbedder

class FAISSVectorStore(VectorStore):
    def __init__(self):
        provider = container.settings.EMBEDDINGS_PROVIDER
        if provider == "openai":
            self.embedder = OpenAIEmbedder()
        else:
            self.embedder = LocalEmbedder(container.settings.LOCAL_EMBEDDINGS_MODEL)
        self.index = None
        self.docs: List[Document] = []

    def index(self, docs: List[Document]) -> int:
        embs = self.embedder.embed([d.text for d in docs])
        xb = np.array(embs).astype("float32")
        d = xb.shape[1]
        if self.index is None:
            self.index = faiss.IndexFlatIP(d)
        self.index.add(xb)
        self.docs.extend(docs)
        return len(docs)

    def search(self, q: Query) -> List[ScoredDocument]:
        if not self.index:
            return []
        emb = np.array(self.embedder.embed([q.text])).astype("float32")
        D, I = self.index.search(emb, q.k)
        results = []
        for score, idx in zip(D[0], I[0]):
            if idx == -1: continue
            results.append(ScoredDocument(doc=self.docs[idx], score=float(score)))
        return results
```

# 3) Retrieval engine (index + query)

## New: backend/domain/retriever.py
```python
from typing import List
from backend.domain.retrieval_types import Document, Query, ScoredDocument
from backend.infra.vecstores.faiss_store import FAISSVectorStore

class Retriever:
    def __init__(self, store=None):
        self.store = store or FAISSVectorStore()

    def index(self, docs: List[Document]) -> int:
        return self.store.index(docs)

    def query(self, q: Query) -> List[ScoredDocument]:
        return self.store.search(q)
```

# 4) Chat orchestration (LLM client + prompt)

## New: backend/infra/llm/base.py
```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class LLM(ABC):
    @abstractmethod
    def generate(self, messages: List[Dict[str,str]], **kwargs) -> str: ...
```

## New: backend/infra/llm/openai_client.py
```python
from typing import List, Dict
from openai import OpenAI
from backend.core.container import container
from backend.infra.llm.base import LLM

class OpenAILLM(LLM):
    def __init__(self, model: str = "gpt-4o-mini"):
        self.client = OpenAI(api_key=container.settings.OPENAI_API_KEY)
        self.model = model

    def generate(self, messages: List[Dict[str,str]], **kwargs) -> str:
        resp = self.client.chat.completions.create(model=self.model, messages=messages, temperature=kwargs.get("temperature", 0.7))
        return resp.choices[0].message.content
```

## New: backend/domain/chat_orchestrator.py
```python
from typing import List, Dict
from backend.domain.retriever import Retriever
from backend.domain.retrieval_types import Query, Document
from backend.infra.llm.openai_client import OpenAILLM

SYSTEM = "Tu es un conseiller astrologique. Tu expliques avec clarté, prudence et bienveillance. Sois concret, évite le jargon. Mentionne la précision si elle est basse."

def build_context_snippets(scored_docs):
    lines = []
    for s in scored_docs:
        lines.append(f"- {s.doc.text}")
    return "\n".join(lines[:6])

class ChatOrchestrator:
    def __init__(self, retriever=None, llm=None):
        self.retriever = retriever or Retriever()
        self.llm = llm or OpenAILLM()

    def advise(self, chart: dict, today: dict, question: str) -> str:
        base = f"User chart precision={chart['chart'].get('precision_score',1)}. EAO={today.get('eao')}."
        r = self.retriever.query(Query(text=question, k=6))
        ctx = build_context_snippets(r)
        messages = [
            {"role":"system", "content": SYSTEM},
            {"role":"user", "content": f"{base}\nQuestion: {question}\nContext:\n{ctx}"},
        ]
        return self.llm.generate(messages)
```

# 5) API route (chat) + entitlement gating

## New: backend/api/routes_chat.py
```python
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from backend.core.container import container
from backend.api.routes_auth import get_current_user
from backend.domain.entitlements import require_entitlement
from backend.domain.chat_orchestrator import ChatOrchestrator

router = APIRouter(prefix="/chat", tags=["chat"])
orch = ChatOrchestrator()

class ChatPayload(BaseModel):
    chart_id: str
    question: str

@router.post("/advise")
def advise(payload: ChatPayload, user=Depends(get_current_user)):
    require_entitlement(user, "plus")
    chart = container.chart_repo.get(payload.chart_id)
    if not chart:
        raise HTTPException(status_code=404, detail="chart_not_found")
    # Optional: compute today's data again or reuse service
    from backend.domain.services import HoroscopeService
    service = HoroscopeService(container.astro, container.content_repo, container.chart_repo)
    today = service.get_today(payload.chart_id)
    text = orch.advise(chart, today, payload.question)
    return {"answer": text, "date": today["date"]}
```

## Edit: backend/app/main.py (include chat router)
```python
from backend.api.routes_chat import router as chat_router
app.include_router(chat_router)
```

# 6) Async infra (Celery)

## New: backend/app/celery_app.py
```python
import os
from celery import Celery
from backend.core.container import container

celery_app = Celery(
    "horoscope",
    broker=container.settings.CELERY_BROKER_URL,
    backend=container.settings.CELERY_RESULT_BACKEND,
)
celery_app.conf.task_routes = {"backend.tasks.*": {"queue": "default"}}
```

## New: backend/tasks/pdf_tasks.py
```python
from backend.app.celery_app import celery_app
from backend.domain.pdf_service import render_natal_pdf
from backend.core.container import container

@celery_app.task(name="backend.tasks.render_pdf")
def render_pdf_task(chart_id: str) -> str:
    chart = container.chart_repo.get(chart_id)
    if not chart: 
        return "not_found"
    pdf_bytes = render_natal_pdf(chart)
    key = f"pdf:natal:{chart_id}"
    try:
        if container.settings.REDIS_URL:
            container.user_repo.client.setex(key, 86400, pdf_bytes)
    except Exception:
        pass
    return "ok"
```

# 7) Monitoring (Prometheus + traces)

## Edit: backend/app/metrics.py (add retrieval/llm metrics)
```python
from prometheus_client import Counter, Histogram

RETRIEVAL_LATENCY = Histogram("retrieval_duration_seconds", "Retrieval latency")
LLM_TOKENS = Counter("llm_tokens_total", "LLM tokens (approx)")
```

## New: backend/app/tracing.py
```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from backend.core.container import container

def setup_tracing():
    if not container.settings.OTLP_ENDPOINT:
        return
    provider = TracerProvider()
    processor = BatchSpanProcessor(OTLPSpanExporter(endpoint=container.settings.OTLP_ENDPOINT, insecure=True))
    provider.add_span_processor(processor)
    trace.set_tracer_provider(provider)
```

## Edit: backend/app/main.py (init tracing early)
```python
from backend.app.tracing import setup_tracing
setup_tracing()
```

# 8) Ingestion script

## New: backend/scripts/ingest_content.py
```python
import json, os, uuid
from backend.domain.retrieval_types import Document
from backend.domain.retriever import Retriever

def main():
    path = os.path.join(os.path.dirname(__file__), "..", "infra", "content.json")
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    docs = []
    for k, v in data.items():
        text = v.get("text","")
        if not text: continue
        docs.append(Document(id=k, text=text, meta={"type":"snippet"}))
    retriever = Retriever()
    n = retriever.index(docs)
    print(f"Indexed {n} docs")

if __name__ == "__main__":
    main()
```

# 9) Tests

## New: backend/tests/fakes.py
```python
from backend.infra.embeddings.base import Embeddings
from backend.infra.llm.base import LLM

class FakeEmbeddings(Embeddings):
    def embed(self, texts):
        # simple length-based vector for determinism
        return [[len(t), 1.0] for t in texts]

class FakeLLM(LLM):
    def generate(self, messages, **kwargs):
        return "FAKE_ADVICE_OK"
```

## New: backend/tests/test_retrieval.py
```python
from backend.domain.retriever import Retriever
from backend.domain.retrieval_types import Document, Query
from backend.infra.vecstores.faiss_store import FAISSVectorStore
from backend.tests.fakes import FakeEmbeddings

def test_retrieval_deterministic(monkeypatch):
    store = FAISSVectorStore()
    # patch embedder
    store.embedder = FakeEmbeddings()
    store.index([Document(id="a", text="alpha"), Document(id="b", text="beta")])
    res = store.search(Query(text="aaa", k=1))
    assert len(res) == 1
```

## New: backend/tests/test_chat.py
```python
from fastapi.testclient import TestClient
from backend.app.main import app
from backend.domain.chat_orchestrator import ChatOrchestrator
from backend.tests.fakes import FakeLLM
from backend.core.container import container

def test_chat_advise_flow(monkeypatch):
    orch = ChatOrchestrator(llm=FakeLLM())
    # create chart
    c = TestClient(app)
    r = c.post("/horoscope/natal", json={"name":"T","date":"1990-01-01","time":None,"tz":"Europe/Paris","lat":48.85,"lon":2.35,"time_certainty":"exact"})
    chart_id = r.json()["id"]
    # call route (bypass auth here or simulate a token if needed)
    data = {"chart_id": chart_id, "question":"Comment optimiser ma journée ?"}
    r2 = c.post("/chat/advise", json=data, headers={})  # if protected, provide Authorization
    assert r2.status_code in (200, 401, 403)  # route gating may apply
```

# 10) Docker & env

## Edit: docker/docker-compose.yml
```yaml
services:
  api:
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - EMBEDDINGS_PROVIDER=${EMBEDDINGS_PROVIDER:-openai}
      - EMBEDDINGS_MODEL=${EMBEDDINGS_MODEL:-text-embedding-3-small}
      - LOCAL_EMBEDDINGS_MODEL=${LOCAL_EMBEDDINGS_MODEL:-all-MiniLM-L6-v2}
      - VECTOR_BACKEND=${VECTOR_BACKEND:-faiss}
      - OTLP_ENDPOINT=${OTLP_ENDPOINT}
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    depends_on:
      - redis
      - worker

  worker:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    command: celery -A backend.app.celery_app.celery_app worker -l INFO -Q default
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    depends_on:
      - redis

  redis:
    image: redis:7-alpine
```

# 11) Acceptance checklist

- [ ] `pytest -q` passes, no flaky tests.
- [ ] `python -m backend.scripts.ingest_content` indexes snippets (“Indexed N docs”).
- [ ] `POST /chat/advise` (with Bearer token + entitlement `plus`) returns an answer.
- [ ] Prometheus `/metrics` exposes `retrieval_duration_seconds` and increments on chat calls.
- [ ] Celery worker receives and completes `backend.tasks.render_pdf` on demand.
- [ ] No import outside `from backend...` and all packages have `__init__.py`.
- [ ] README updated with run instructions and environment variables.

# End
Please apply exactly these steps and code files. Keep style consistent with existing codebase and ensure deterministic tests by using fakes where appropriate.
